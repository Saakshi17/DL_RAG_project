{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "import os\n",
    "import langchain\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"]='sk-'\n",
    "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arxiv\n",
    "!pip install -U langchain\n",
    "!pip install -U langchain_community\n",
    "!pip install -U langchain_openai\n",
    "!pip install faiss-cpu\n",
    "!pip install pypdf\n",
    "!pip install transformers\n",
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs=loader.load()\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "vectordb=FAISS.from_documents(documents,OpenAIEmbeddings())\n",
    "retriever=vectordb.as_retriever()\n",
    "retriever\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(retriever,\"langsmith_search\",\n",
    "                      \"Search for information \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki,arxiv,retriever_tool]\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo',temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchainhub in c:\\users\\gupta\\appdata\\roaming\\python\\python39\\site-packages (0.1.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\gupta\\appdata\\roaming\\python\\python39\\site-packages (from langchainhub) (2.31.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in c:\\users\\gupta\\appdata\\roaming\\python\\python39\\site-packages (from langchainhub) (2.31.0.20240406)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gupta\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gupta\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2->langchainhub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gupta\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2->langchainhub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gupta\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install langchainhub\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `Neural networks in deep learning`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Deep learning\n",
      "Summary: Deep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \"deep\" refers to the use of \u001b[0m\u001b[32;1m\u001b[1;3mNeural networks in deep learning are a subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The term \"deep\" in deep learning refers to the use of multiple layers in the neural network architecture. These networks are designed to automatically learn and improve from experience without being explicitly programmed.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are neural networks in deep learning',\n",
       " 'output': 'Neural networks in deep learning are a subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The term \"deep\" in deep learning refers to the use of multiple layers in the neural network architecture. These networks are designed to automatically learn and improve from experience without being explicitly programmed.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### Agents\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,prompt)\n",
    "\n",
    "## Agent Executer\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor\n",
    "agent_executor.invoke({\"input\":\"What are neural networks in deep learning\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
